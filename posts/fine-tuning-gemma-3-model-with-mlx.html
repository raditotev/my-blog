<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><meta name="og:title" content="Fine-Tuning Google&#x27;s Gemma-3-4B-IT Model with MLX_LM and Preparing It for Ollama" class="jsx-eb7a88ca53252729"/><title class="jsx-eb7a88ca53252729">Fine-Tuning Google&#x27;s Gemma-3-4B-IT Model with MLX_LM and Preparing It for Ollama</title><meta name="next-head-count" content="4"/><link rel="icon" href="/favicon/favicon.ico"/><meta name="description" content="A collection of technical posts."/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H1DT8LBE3Y"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H1DT8LBE3Y');
          </script><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-5f4595e5518b5600.js" defer=""></script><script src="/_next/static/chunks/main-f65e66e62fc5ca80.js" defer=""></script><script src="/_next/static/chunks/pages/_app-02d0f4839caa4a8e.js" defer=""></script><script src="/_next/static/chunks/83-7030382a9597fa38.js" defer=""></script><script src="/_next/static/chunks/32-1cf93db441761c8a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-7655989d47098e38.js" defer=""></script><script src="/_next/static/BuKLCWQig9E5xWXSfwVPc/_buildManifest.js" defer=""></script><script src="/_next/static/BuKLCWQig9E5xWXSfwVPc/_ssgManifest.js" defer=""></script><script src="/_next/static/BuKLCWQig9E5xWXSfwVPc/_middlewareManifest.js" defer=""></script><style id="__jsx-fdefdd4b6c4ce2c4">span.jsx-fdefdd4b6c4ce2c4{color:#333;font-size:.9rem}</style><style id="__jsx-7814f24efc48ee15">.list-categories.jsx-7814f24efc48ee15 a.jsx-7814f24efc48ee15{color:#333;text-transform:uppercase;font-size:.6rem;font-weight:700;margin-right:10px}</style><style id="__jsx-eb7a88ca53252729">.post.jsx-eb7a88ca53252729{width:100%}header.jsx-eb7a88ca53252729{text-align:center;margin-bottom:3rem}.post.jsx-eb7a88ca53252729 h1.jsx-eb7a88ca53252729{margin-bottom:.2rem}a.jsx-eb7a88ca53252729{color:black}footer.jsx-eb7a88ca53252729{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-moz-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}footer.jsx-eb7a88ca53252729 a.go-back.jsx-eb7a88ca53252729{font-size:1.5rem}@media(max-width:820px){footer.jsx-eb7a88ca53252729{display:block}footer.jsx-eb7a88ca53252729 a.go-back.jsx-eb7a88ca53252729{float:right}}</style><style id="__jsx-5cae0e9dd922305c">html,body{padding:0;margin:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;font-size:20px}*{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}p{font-weight:400;line-height:32px}a{text-decoration:none}img{width:100%}img[src*="#center"]{display:block;margin:auto}@media(max-width:820px){body{font-size:18px}p{line-height:28px}img{max-width:80vw}}</style><style id="__jsx-53df2e978e8b299a">header.jsx-5a195dee6678eb3{width:100vw;padding:1.7rem;background-color:rgba(0,0,0,.9);text-align:center}header.jsx-5a195dee6678eb3 h1.jsx-5a195dee6678eb3{margin:0}header.jsx-5a195dee6678eb3 a.jsx-5a195dee6678eb3{text-decoration:none;color:crimson}main.jsx-5a195dee6678eb3{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;-moz-box-orient:vertical;-moz-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-moz-box-pack:center;-ms-flex-pack:center;justify-content:center;margin:20px auto;max-width:820px}@media(max-width:820px){main.jsx-5a195dee6678eb3{width:95vw}}</style></head><body><div id="__next" data-reactroot=""><div class="jsx-5a195dee6678eb3"><header class="jsx-5a195dee6678eb3"><div class="jsx-5a195dee6678eb3"><a class="jsx-5a195dee6678eb3" href="/"><h1 class="jsx-5a195dee6678eb3">Radi&#x27;s Blog</h1></a></div></header><main class="jsx-5a195dee6678eb3"><div class="jsx-eb7a88ca53252729 post"><header class="jsx-eb7a88ca53252729"><h1 class="jsx-eb7a88ca53252729">Fine-Tuning Google&#x27;s Gemma-3-4B-IT Model with MLX_LM and Preparing It for Ollama</h1><span class="jsx-fdefdd4b6c4ce2c4"><time dateTime="2025-11-21T00:00:00.000Z" class="jsx-fdefdd4b6c4ce2c4">21 November 2025</time></span></header><h1>Fine-Tuning Google&#x27;s Gemma-3-4B-IT Model with MLX_LM and Preparing It for Ollama</h1>
<p>Fine-tuning large language models (LLMs) has become increasingly accessible, thanks to tools like MLX_LM, which is optimized for Apple Silicon. In this blog post, I&#x27;ll walk you through the process of fine-tuning the base Google Gemma-3-4B-IT model using a custom dataset, fusing the adapters, and converting the result to GGUF format for use with Ollama. This guide is based on my personal notes from a recent experiment, and it&#x27;s aimed at anyone looking to customize an LLM for specific tasks.</p>
<p>We&#x27;ll cover the key steps: downloading the base model, fine-tuning, fusing, handling the tokenizer, conversion to GGUF, and setting up a Modelfile for Ollama. Assumptions include owning a Mac M-series, having MLX_LM installed, access to Hugging Face models, and Llama.cpp for the conversion step. Let&#x27;s dive in!</p>
<h2>Step 1: Downloading the Base Model from Hugging Face</h2>
<p>Before you can fine-tune the model, you need to download the base Google Gemma-3-4B-IT model from Hugging Face. Note that Gemma models are gated, meaning you must accept the license terms on the Hugging Face model page (https://huggingface.co/google/gemma-3-4b-it) and have an approved account. Once accepted, you&#x27;ll need a Hugging Face access token for authenticated downloads.</p>
<p>First, ensure you have Homebrew installed on your macOS system (if not, install it from https://brew.sh/). Then, install the Hugging Face CLI using Homebrew:</p>
<pre><div style="color:white;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto;background:#011627"><code class="language-bash" style="color:#d6deeb;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>brew </span><span class="token" style="color:rgb(130, 170, 255)">install</span><span> huggingface-cli</span></code></div></pre>
<p>To get your credentials for logging in:</p>
<ol>
<li>Go to https://huggingface.co/ and sign up for a free account if you don&#x27;t have one.</li>
<li>Navigate to your profile settings by clicking on your avatar in the top right and selecting &quot;Settings.&quot;</li>
<li>In the left sidebar, click on &quot;Access Tokens.&quot;</li>
<li>Click &quot;New token&quot; to generate a new access token. Give it a name, select the appropriate scopes (e.g., &quot;read&quot; for downloading models), and copy the generated token. If you only want to download the model select &quot;read&quot; but if you want later on to upload you fine-tuned model to huggingface select the &quot;write&quot; option.</li>
</ol>
<p>Then, log in to Hugging Face using the CLI:</p>
<pre><div style="color:white;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto;background:#011627"><code class="language-bash" style="color:#d6deeb;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>huggingface-cli login</span></code></div></pre>
<p>Paste your access token when prompted.</p>
<p>Now, download the model:</p>
<pre><div style="color:white;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto;background:#011627"><code class="language-bash" style="color:#d6deeb;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>huggingface-cli download google/gemma-3-4b-it --local-dir gemma-3-4b-it</span></code></div></pre>
<p>This saves the model files to a local directory named <code style="font-size:1rem;color:crimson">gemma-3-4b-it</code>. Alternatively, if you skip this step, MLX_LM will automatically download the model to your cache (<code style="font-size:1rem;color:crimson">~/.cache/huggingface/hub</code>) when you run the fine-tuning command in the next step. However, explicit downloading ensures everything is ready and allows you to verify the files.</p>
<h1>Step 2: Preparing the data set</h1>
<p>In order to fine-tune the model you&#x27;ll need a dataset. There are three different types of data formats mlx <a href="https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/LORA.md#Data">supports</a> and after testing each one of them I&#x27;ve found I get the best results with the <code style="font-size:1rem;color:crimson">completion</code> format, e.g.:</p>
<pre><code style="font-size:1rem;color:crimson">{&quot;prompt&quot;: &quot;What is the capital of France?&quot;, &quot;completion&quot;: &quot;Paris.&quot;}
</code></pre>
<p>To get any meaningful result you&#x27;ll need 50 or more of those in a <code style="font-size:1rem;color:crimson">train.jsonl</code> file. It&#x27;s important to notice the train file is <a href="https://jsonlines.org/">JSONL</a>. This is different from json. In this format every object is on a new line and there are no comma separators. You&#x27;ll need a <code style="font-size:1rem;color:crimson">valid.jsonl</code> file as well were you&#x27;ll have the same entries like the <code style="font-size:1rem;color:crimson">train.jsonl</code>. The rule of thumb is to have 80% of the dataset in train.jsonl and 20% in <code style="font-size:1rem;color:crimson">valid.jsonl</code>.</p>
<h2>Step 3: Fine-Tuning the Model</h2>
<p>With the base model available, proceed to fine-tune it using your dataset. In this example, we&#x27;re using the Google Gemma-3-4B-IT as the base, with training data located in a <code style="font-size:1rem;color:crimson">./data</code> folder. MLX_LM&#x27;s LoRA (Low-Rank Adaptation) method makes this efficient without retraining the entire model.</p>
<p>Run the following command:</p>
<pre><div style="color:white;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto;background:#011627"><code class="language-bash" style="color:#d6deeb;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>mlx_lm.lora --model google/gemma-3-4b-it --train --data ./data --batch-size </span><span class="token" style="color:rgb(247, 140, 108)">4</span><span> --iters </span><span class="token" style="color:rgb(247, 140, 108)">100</span></code></div></pre>
<ul>
<li><code style="font-size:1rem;color:crimson">--model</code>: Specifies the base model from Hugging Face (or your local path if downloaded manually).</li>
<li><code style="font-size:1rem;color:crimson">--train</code>: Enables training mode.</li>
<li><code style="font-size:1rem;color:crimson">--data</code>: Path to your dataset folder (e.g., JSONL files with prompt-completion pairs).</li>
<li><code style="font-size:1rem;color:crimson">--batch-size 4</code>: Keeps memory usage manageable.</li>
<li><code style="font-size:1rem;color:crimson">--iters 100</code>: Number of training iterations‚Äîadjust based on your dataset size and convergence.</li>
</ul>
<p>This will generate adapter files in an <code style="font-size:1rem;color:crimson">adapters</code> directory. Monitor the output for loss metrics to gauge training progress.</p>
<h2>Step 4: Fusing the Adapters with the Base Model</h2>
<p>Once fine-tuning is complete, fuse the adapters back into the base model to create a single, unified fine-tuned model.</p>
<p>Use this command:</p>
<pre><div style="color:white;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto;background:#011627"><code class="language-bash" style="color:#d6deeb;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>mlx_lm.fuse --model google/gemma-3-4b-it --save-path gemma-3-4b-it-ft --adapter-path adapters --de-quantize</span></code></div></pre>
<ul>
<li><code style="font-size:1rem;color:crimson">--save-path</code>: Directory where the fused model will be saved (e.g., <code style="font-size:1rem;color:crimson">gemma-3-4b-it-ft</code>).</li>
<li><code style="font-size:1rem;color:crimson">--adapter-path</code>: Path to the adapters from Step 2.</li>
<li><code style="font-size:1rem;color:crimson">--de-quantize</code>: Ensures the model is in full precision (useful for further conversions).</li>
</ul>
<p>This step combines the learned adaptations with the original weights, resulting in a ready-to-use fine-tuned model.</p>
<h2>Step 5: Copying the Tokenizer</h2>
<blockquote>
<p>[!NOTE] I didn&#x27;t have to do this step when fine-tuning Llama-3.2-3B-Instruct</p>
</blockquote>
<p>Unlike some frameworks (e.g., Ollama&#x27;s built-in fine-tuning), MLX_LM doesn&#x27;t automatically include the tokenizer in the fused model. You&#x27;ll need to copy it manually from the base model&#x27;s cache.</p>
<p>First, locate the tokenizer in your Hugging Face cache (replace <code style="font-size:1rem;color:crimson">&lt;commit hash&gt;</code> with the actual hash from your local cache):</p>
<pre><div style="color:white;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto;background:#011627"><code class="language-bash" style="color:#d6deeb;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token" style="color:rgb(130, 170, 255)">cp</span><span> ~/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/</span><span class="token" style="color:rgb(127, 219, 202)">&lt;</span><span>commit hash</span><span class="token" style="color:rgb(127, 219, 202)">&gt;</span><span>/tokenizer.model gemma-3-4b-it-ft</span></code></div></pre>
<p>This ensures the fused model has the correct tokenizer for text processing.</p>
<h2>Step 6: Converting to GGUF Format</h2>
<p>To use the model with Ollama, convert it to GGUF (a format supported by Llama.cpp and Ollama). This requires the Llama.cpp repository cloned and set up.</p>
<p>Navigate to the Llama.cpp directory and run:</p>
<pre><div style="color:white;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto;background:#011627"><code class="language-bash" style="color:#d6deeb;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>python convert_hf_to_gguf.py path-to-your-project/gemma-3-4b-it-ft --outfile </span><span class="token" style="color:rgb(199, 146, 234)">..</span><span>/gguf/</span><span class="token" style="color:rgb(127, 219, 202)">&lt;</span><span>name your model</span><span class="token" style="color:rgb(127, 219, 202)">&gt;</span><span>.gguf --outtype f16</span></code></div></pre>
<ul>
<li>Path to fused model: Adjust based on your directory structure (e.g., <code style="font-size:1rem;color:crimson">path-to-your-project/gemma-3-4b-it-ft</code>).</li>
<li><code style="font-size:1rem;color:crimson">--outfile</code>: Path to save the GGUF file.</li>
<li><code style="font-size:1rem;color:crimson">--outtype f16</code>: Uses float16 precision for a balance of size and performance.</li>
</ul>
<p>This conversion makes the model compatible with Ollama&#x27;s ecosystem.</p>
<h2>Step 7: Creating a Modelfile and Running in Ollama</h2>
<p>Finally, set up a Modelfile to define how Ollama should load and interact with your model. Create a file named <code style="font-size:1rem;color:crimson">Modelfile</code> with the following content:</p>
<pre><code style="font-size:1rem;color:crimson">FROM ./gguf/&lt;name of you model&gt;.gguf

TEMPLATE &quot;&quot;&quot;{{ .System }}
{{ .Prompt }}
{{ .Response }}&quot;&quot;&quot;

PARAMETER num_ctx 2048
PARAMETER temperature 0.7
</code></pre>
<ul>
<li><code style="font-size:1rem;color:crimson">FROM</code>: Points to your GGUF file.</li>
<li><code style="font-size:1rem;color:crimson">TEMPLATE</code>: Defines the chat format (customize based on your needs).</li>
<li><code style="font-size:1rem;color:crimson">PARAMETER</code>: Sets context window and generation parameters.</li>
</ul>
<p>To create and run the model in Ollama:</p>
<pre><div style="color:white;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto;background:#011627"><code class="language-bash" style="color:#d6deeb;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;font-size:1em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>ollama create my-fine-tuned-gemma -f Modelfile
</span>ollama run my-fine-tuned-gemma</code></div></pre>
<p>Now you can interact with your fine-tuned model via Ollama&#x27;s UI, CLI or API!</p>
<h2>Conclusion</h2>
<p>Fine-tuning with MLX_LM is a powerful way to adapt models like Gemma-3-4B-IT to your data, and integrating with Ollama opens up easy deployment options. This process took me just a few hours on an M3 Mac, but results will vary based on hardware and dataset. Experiment with hyperparameters, and always evaluate for biases or hallucinations post-fine-tuning.</p>
<p>If you run into issues, check the MLX_LM docs or Llama.cpp GitHub. Happy fine-tuning! üöÄ</p>
<h2>Resources</h2>
<p><a href="https://github.com/ggml-org/llama.cpp">Llama.cpp</a><br/>
<a href="https://github.com/ml-explore/mlx-lm">MLX-LM</a><br/>
<a href="https://huggingface.co/">HuggingFace</a><br/>
<a href="https://jsonlines.org/">JSONL</a></p><footer class="jsx-eb7a88ca53252729"><div class="jsx-7814f24efc48ee15 list-categories"><a class="jsx-7814f24efc48ee15" href="/categories/ai">ai</a><a class="jsx-7814f24efc48ee15" href="/categories/macos">macos</a></div><a class="jsx-eb7a88ca53252729 go-back" href="/">üè†</a></footer></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"id":"fine-tuning-gemma-3-model-with-mlx","title":"Fine-Tuning Google's Gemma-3-4B-IT Model with MLX_LM and Preparing It for Ollama","date":"2025-11-21T00:00:00.000Z","description":"The article is a blog post that offers a detailed, step-by-step guide to fine-tuning Google's Gemma-3-4B-IT model using MLX_LM on Apple Silicon. It covers downloading the base model from Hugging Face, training with LoRA adapters and a custom dataset, fusing the adapters, copying the tokenizer, converting the model to GGUF format for compatibility with Ollama, and setting up a Modelfile for deployment.","categories":["ai","macos"],"markdown":"\n# Fine-Tuning Google's Gemma-3-4B-IT Model with MLX_LM and Preparing It for Ollama\n\nFine-tuning large language models (LLMs) has become increasingly accessible, thanks to tools like MLX_LM, which is optimized for Apple Silicon. In this blog post, I'll walk you through the process of fine-tuning the base Google Gemma-3-4B-IT model using a custom dataset, fusing the adapters, and converting the result to GGUF format for use with Ollama. This guide is based on my personal notes from a recent experiment, and it's aimed at anyone looking to customize an LLM for specific tasks.\n\nWe'll cover the key steps: downloading the base model, fine-tuning, fusing, handling the tokenizer, conversion to GGUF, and setting up a Modelfile for Ollama. Assumptions include owning a Mac M-series, having MLX_LM installed, access to Hugging Face models, and Llama.cpp for the conversion step. Let's dive in!\n\n## Step 1: Downloading the Base Model from Hugging Face\n\nBefore you can fine-tune the model, you need to download the base Google Gemma-3-4B-IT model from Hugging Face. Note that Gemma models are gated, meaning you must accept the license terms on the Hugging Face model page (https://huggingface.co/google/gemma-3-4b-it) and have an approved account. Once accepted, you'll need a Hugging Face access token for authenticated downloads.\n\nFirst, ensure you have Homebrew installed on your macOS system (if not, install it from https://brew.sh/). Then, install the Hugging Face CLI using Homebrew:\n\n```bash\nbrew install huggingface-cli\n```\n\nTo get your credentials for logging in:\n\n1. Go to https://huggingface.co/ and sign up for a free account if you don't have one.\n2. Navigate to your profile settings by clicking on your avatar in the top right and selecting \"Settings.\"\n3. In the left sidebar, click on \"Access Tokens.\"\n4. Click \"New token\" to generate a new access token. Give it a name, select the appropriate scopes (e.g., \"read\" for downloading models), and copy the generated token. If you only want to download the model select \"read\" but if you want later on to upload you fine-tuned model to huggingface select the \"write\" option.\n\nThen, log in to Hugging Face using the CLI:\n\n```bash\nhuggingface-cli login\n```\n\nPaste your access token when prompted.\n\nNow, download the model:\n\n```bash\nhuggingface-cli download google/gemma-3-4b-it --local-dir gemma-3-4b-it\n```\n\nThis saves the model files to a local directory named `gemma-3-4b-it`. Alternatively, if you skip this step, MLX_LM will automatically download the model to your cache (`~/.cache/huggingface/hub`) when you run the fine-tuning command in the next step. However, explicit downloading ensures everything is ready and allows you to verify the files.\n\n# Step 2: Preparing the data set\n\nIn order to fine-tune the model you'll need a dataset. There are three different types of data formats mlx [supports](https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/LORA.md#Data) and after testing each one of them I've found I get the best results with the `completion` format, e.g.:\n\n```\n{\"prompt\": \"What is the capital of France?\", \"completion\": \"Paris.\"}\n```\n\nTo get any meaningful result you'll need 50 or more of those in a `train.jsonl` file. It's important to notice the train file is [JSONL](https://jsonlines.org/). This is different from json. In this format every object is on a new line and there are no comma separators. You'll need a `valid.jsonl` file as well were you'll have the same entries like the `train.jsonl`. The rule of thumb is to have 80% of the dataset in train.jsonl and 20% in `valid.jsonl`.\n\n## Step 3: Fine-Tuning the Model\n\nWith the base model available, proceed to fine-tune it using your dataset. In this example, we're using the Google Gemma-3-4B-IT as the base, with training data located in a `./data` folder. MLX_LM's LoRA (Low-Rank Adaptation) method makes this efficient without retraining the entire model.\n\nRun the following command:\n\n```bash\nmlx_lm.lora --model google/gemma-3-4b-it --train --data ./data --batch-size 4 --iters 100\n```\n\n- `--model`: Specifies the base model from Hugging Face (or your local path if downloaded manually).\n- `--train`: Enables training mode.\n- `--data`: Path to your dataset folder (e.g., JSONL files with prompt-completion pairs).\n- `--batch-size 4`: Keeps memory usage manageable.\n- `--iters 100`: Number of training iterations‚Äîadjust based on your dataset size and convergence.\n\nThis will generate adapter files in an `adapters` directory. Monitor the output for loss metrics to gauge training progress.\n\n## Step 4: Fusing the Adapters with the Base Model\n\nOnce fine-tuning is complete, fuse the adapters back into the base model to create a single, unified fine-tuned model.\n\nUse this command:\n\n```bash\nmlx_lm.fuse --model google/gemma-3-4b-it --save-path gemma-3-4b-it-ft --adapter-path adapters --de-quantize\n```\n\n- `--save-path`: Directory where the fused model will be saved (e.g., `gemma-3-4b-it-ft`).\n- `--adapter-path`: Path to the adapters from Step 2.\n- `--de-quantize`: Ensures the model is in full precision (useful for further conversions).\n\nThis step combines the learned adaptations with the original weights, resulting in a ready-to-use fine-tuned model.\n\n## Step 5: Copying the Tokenizer\n\n\u003e [!NOTE] I didn't have to do this step when fine-tuning Llama-3.2-3B-Instruct\n\nUnlike some frameworks (e.g., Ollama's built-in fine-tuning), MLX_LM doesn't automatically include the tokenizer in the fused model. You'll need to copy it manually from the base model's cache.\n\nFirst, locate the tokenizer in your Hugging Face cache (replace `\u003ccommit hash\u003e` with the actual hash from your local cache):\n\n```bash\ncp ~/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/\u003ccommit hash\u003e/tokenizer.model gemma-3-4b-it-ft\n```\n\nThis ensures the fused model has the correct tokenizer for text processing.\n\n## Step 6: Converting to GGUF Format\n\nTo use the model with Ollama, convert it to GGUF (a format supported by Llama.cpp and Ollama). This requires the Llama.cpp repository cloned and set up.\n\nNavigate to the Llama.cpp directory and run:\n\n```bash\npython convert_hf_to_gguf.py path-to-your-project/gemma-3-4b-it-ft --outfile ../gguf/\u003cname your model\u003e.gguf --outtype f16\n```\n\n- Path to fused model: Adjust based on your directory structure (e.g., `path-to-your-project/gemma-3-4b-it-ft`).\n- `--outfile`: Path to save the GGUF file.\n- `--outtype f16`: Uses float16 precision for a balance of size and performance.\n\nThis conversion makes the model compatible with Ollama's ecosystem.\n\n## Step 7: Creating a Modelfile and Running in Ollama\n\nFinally, set up a Modelfile to define how Ollama should load and interact with your model. Create a file named `Modelfile` with the following content:\n\n```\nFROM ./gguf/\u003cname of you model\u003e.gguf\n\nTEMPLATE \"\"\"{{ .System }}\n{{ .Prompt }}\n{{ .Response }}\"\"\"\n\nPARAMETER num_ctx 2048\nPARAMETER temperature 0.7\n```\n\n- `FROM`: Points to your GGUF file.\n- `TEMPLATE`: Defines the chat format (customize based on your needs).\n- `PARAMETER`: Sets context window and generation parameters.\n\nTo create and run the model in Ollama:\n\n```bash\nollama create my-fine-tuned-gemma -f Modelfile\nollama run my-fine-tuned-gemma\n```\n\nNow you can interact with your fine-tuned model via Ollama's UI, CLI or API!\n\n## Conclusion\n\nFine-tuning with MLX_LM is a powerful way to adapt models like Gemma-3-4B-IT to your data, and integrating with Ollama opens up easy deployment options. This process took me just a few hours on an M3 Mac, but results will vary based on hardware and dataset. Experiment with hyperparameters, and always evaluate for biases or hallucinations post-fine-tuning.\n\nIf you run into issues, check the MLX_LM docs or Llama.cpp GitHub. Happy fine-tuning! üöÄ\n\n## Resources\n\n[Llama.cpp](https://github.com/ggml-org/llama.cpp)  \n[MLX-LM](https://github.com/ml-explore/mlx-lm)  \n[HuggingFace](https://huggingface.co/)  \n[JSONL](https://jsonlines.org/)\n"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"fine-tuning-gemma-3-model-with-mlx"},"buildId":"BuKLCWQig9E5xWXSfwVPc","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>