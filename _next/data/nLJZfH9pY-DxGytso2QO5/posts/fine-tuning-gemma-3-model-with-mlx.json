{"pageProps":{"post":{"id":"fine-tuning-gemma-3-model-with-mlx","title":"Fine-Tuning Google's Gemma-3-4B-IT Model with MLX_LM and Preparing It for Ollama","date":"2025-11-21T00:00:00.000Z","description":"The article is a blog post that offers a detailed, step-by-step guide to fine-tuning Google's Gemma-3-4B-IT model using MLX_LM on Apple Silicon. It covers downloading the base model from Hugging Face, training with LoRA adapters and a custom dataset, fusing the adapters, copying the tokenizer, converting the model to GGUF format for compatibility with Ollama, and setting up a Modelfile for deployment.","categories":["ai","macos"],"markdown":"\n# Fine-Tuning Google's Gemma-3-4B-IT Model with MLX_LM and Preparing It for Ollama\n\nFine-tuning large language models (LLMs) has become increasingly accessible, thanks to tools like MLX_LM, which is optimized for Apple Silicon. In this blog post, I'll walk you through the process of fine-tuning the base Google Gemma-3-4B-IT model using a custom dataset, fusing the adapters, and converting the result to GGUF format for use with Ollama. This guide is based on my personal notes from a recent experiment, and it's aimed at anyone looking to customize an LLM for specific tasks.\n\nWe'll cover the key steps: downloading the base model, fine-tuning, fusing, handling the tokenizer, conversion to GGUF, and setting up a Modelfile for Ollama. Assumptions include owning a Mac M-series, having MLX_LM installed, access to Hugging Face models, and Llama.cpp for the conversion step. Let's dive in!\n\n## Step 1: Downloading the Base Model from Hugging Face\n\nBefore you can fine-tune the model, you need to download the base Google Gemma-3-4B-IT model from Hugging Face. Note that Gemma models are gated, meaning you must accept the license terms on the Hugging Face model page (https://huggingface.co/google/gemma-3-4b-it) and have an approved account. Once accepted, you'll need a Hugging Face access token for authenticated downloads.\n\nFirst, ensure you have Homebrew installed on your macOS system (if not, install it from https://brew.sh/). Then, install the Hugging Face CLI using Homebrew:\n\n```bash\nbrew install huggingface-cli\n```\n\nTo get your credentials for logging in:\n\n1. Go to https://huggingface.co/ and sign up for a free account if you don't have one.\n2. Navigate to your profile settings by clicking on your avatar in the top right and selecting \"Settings.\"\n3. In the left sidebar, click on \"Access Tokens.\"\n4. Click \"New token\" to generate a new access token. Give it a name, select the appropriate scopes (e.g., \"read\" for downloading models), and copy the generated token. If you only want to download the model select \"read\" but if you want later on to upload you fine-tuned model to huggingface select the \"write\" option.\n\nThen, log in to Hugging Face using the CLI:\n\n```bash\nhuggingface-cli login\n```\n\nPaste your access token when prompted.\n\nNow, download the model:\n\n```bash\nhuggingface-cli download google/gemma-3-4b-it --local-dir gemma-3-4b-it\n```\n\nThis saves the model files to a local directory named `gemma-3-4b-it`. Alternatively, if you skip this step, MLX_LM will automatically download the model to your cache (`~/.cache/huggingface/hub`) when you run the fine-tuning command in the next step. However, explicit downloading ensures everything is ready and allows you to verify the files.\n\n# Step 2: Preparing the data set\n\nIn order to fine-tune the model you'll need a dataset. There are three different types of data formats mlx [supports](https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/LORA.md#Data) and after testing each one of them I've found I get the best results with the `completion` format, e.g.:\n\n```\n{\"prompt\": \"What is the capital of France?\", \"completion\": \"Paris.\"}\n```\n\nTo get any meaningful result you'll need 50 or more of those in a `train.jsonl` file. It's important to notice the train file is [JSONL](https://jsonlines.org/). This is different from json. In this format every object is on a new line and there are no comma separators. You'll need a `valid.jsonl` file as well were you'll have the same entries like the `train.jsonl`. The rule of thumb is to have 80% of the dataset in train.jsonl and 20% in `valid.jsonl`.\n\n## Step 3: Fine-Tuning the Model\n\nWith the base model available, proceed to fine-tune it using your dataset. In this example, we're using the Google Gemma-3-4B-IT as the base, with training data located in a `./data` folder. MLX_LM's LoRA (Low-Rank Adaptation) method makes this efficient without retraining the entire model.\n\nRun the following command:\n\n```bash\nmlx_lm.lora --model google/gemma-3-4b-it --train --data ./data --batch-size 4 --iters 100\n```\n\n- `--model`: Specifies the base model from Hugging Face (or your local path if downloaded manually).\n- `--train`: Enables training mode.\n- `--data`: Path to your dataset folder (e.g., JSONL files with prompt-completion pairs).\n- `--batch-size 4`: Keeps memory usage manageable.\n- `--iters 100`: Number of training iterationsâ€”adjust based on your dataset size and convergence.\n\nThis will generate adapter files in an `adapters` directory. Monitor the output for loss metrics to gauge training progress.\n\n## Step 4: Fusing the Adapters with the Base Model\n\nOnce fine-tuning is complete, fuse the adapters back into the base model to create a single, unified fine-tuned model.\n\nUse this command:\n\n```bash\nmlx_lm.fuse --model google/gemma-3-4b-it --save-path gemma-3-4b-it-ft --adapter-path adapters --de-quantize\n```\n\n- `--save-path`: Directory where the fused model will be saved (e.g., `gemma-3-4b-it-ft`).\n- `--adapter-path`: Path to the adapters from Step 2.\n- `--de-quantize`: Ensures the model is in full precision (useful for further conversions).\n\nThis step combines the learned adaptations with the original weights, resulting in a ready-to-use fine-tuned model.\n\n## Step 5: Copying the Tokenizer\n\n> [!NOTE]\n> I didn't have to do this step when fine-tuning Llama-3.2-3B-Instruct\n\nUnlike some frameworks (e.g., Ollama's built-in fine-tuning), MLX_LM doesn't automatically include the tokenizer in the fused model. You'll need to copy it manually from the base model's cache.\n\nFirst, locate the tokenizer in your Hugging Face cache (replace `<commit hash>` with the actual hash from your local cache):\n\n```bash\ncp ~/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/<commit hash>/tokenizer.model gemma-3-4b-it-ft\n```\n\nThis ensures the fused model has the correct tokenizer for text processing.\n\n## Step 6: Converting to GGUF Format\n\nTo use the model with Ollama, convert it to GGUF (a format supported by Llama.cpp and Ollama). This requires the Llama.cpp repository cloned and set up.\n\nNavigate to the Llama.cpp directory and run:\n\n```bash\npython convert_hf_to_gguf.py path-to-your-project/gemma-3-4b-it-ft --outfile ../gguf/<name your model>.gguf --outtype f16\n```\n\n- Path to fused model: Adjust based on your directory structure (e.g., `path-to-your-project/gemma-3-4b-it-ft`).\n- `--outfile`: Path to save the GGUF file.\n- `--outtype f16`: Uses float16 precision for a balance of size and performance.\n\nThis conversion makes the model compatible with Ollama's ecosystem.\n\n## Step 7: Creating a Modelfile and Running in Ollama\n\nFinally, set up a Modelfile to define how Ollama should load and interact with your model. Create a file named `Modelfile` with the following content:\n\n```\nFROM ./gguf/<name of you model>.gguf\n\nTEMPLATE \"\"\"{{ .System }}\n{{ .Prompt }}\n{{ .Response }}\"\"\"\n\nPARAMETER num_ctx 2048\nPARAMETER temperature 0.7\n```\n\n- `FROM`: Points to your GGUF file.\n- `TEMPLATE`: Defines the chat format (customize based on your needs).\n- `PARAMETER`: Sets context window and generation parameters.\n\nTo create and run the model in Ollama:\n\n```bash\nollama create my-fine-tuned-gemma -f Modelfile\nollama run my-fine-tuned-gemma\n```\n\nNow you can interact with your fine-tuned model via Ollama's UI, CLI or API!\n\n## Conclusion\n\nFine-tuning with MLX_LM is a powerful way to adapt models like Gemma-3-4B-IT to your data, and integrating with Ollama opens up easy deployment options. This process took me just a few hours on an M3 Mac, but results will vary based on hardware and dataset. Experiment with hyperparameters, and always evaluate for biases or hallucinations post-fine-tuning.\n\nIf you run into issues, check the MLX_LM docs or Llama.cpp GitHub. Happy fine-tuning! ðŸš€\n\n## Resources\n\n[Llama.cpp](https://github.com/ggml-org/llama.cpp)  \n[MLX-LM](https://github.com/ml-explore/mlx-lm)  \n[HuggingFace](https://huggingface.co/)  \n[JSONL](https://jsonlines.org/)\n"}},"__N_SSG":true}